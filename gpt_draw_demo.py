# -*- coding: utf-8 -*-
"""GPT-Draw Demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOiYAX_lNiIOCya3NvQBMtO8GT2s-kEr

# GPT-Draw: Generate images using GPT-2

Try GPT-Draw, an autoregressive text-to-image generator based on the GPT-2 language model. It was trained from scratch to generate "image tokens" instead of text tokens.

Here, you can run the model and try your own prompts. Keep in mind that the model is small and also trained on a small dataset, so don't expect great performance on difficult prompts.

- GPT parameters: 336M
- VQGAN parameters: 85M
- CLIP parameters: 151M
- training dataset size: 2.7M

For more training details and how this works, check the report on Weights and Biases (https://wandb.ai/bcs1/gptgen/reports/pretraining-GPT-2-for-text-to-image--Vmlldzo1MzU4NTk3).

Discord: Heeusk#1224

### Make sure you have a GPU
"""

!nvidia-smi

"""### Imports and model setup

This downloads the models and installs everything needed and should take around 5 minutes on Colab.
"""

!pip install transformers accelerate
#!pip install taming-transformers
!pip install git+https://github.com/openai/CLIP.git

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import numpy as np
import clip
import yaml

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained("ouasdg/gpt-draw", padding=False)
model = AutoModelForCausalLM.from_pretrained("ouasdg/gpt-draw")

model.to(device)
model.eval()

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/oupak/taming

!pip install ftfy regex tqdm omegaconf pytorch-lightning
!pip install einops
!pip install wget

# %cd taming
!wget -O model.zip https://ommer-lab.com/files/latent-diffusion/vq-f16.zip
!unzip -o model.zip

# vqgan model setup
from taming.taming.models import vqgan

with open("config.yaml", "r") as config_file:
    config = yaml.safe_load(config_file)

ddconfig = config["ddconfig"]
lossconfig = config["lossconfig"]
n_embed = config["n_embed"]
embed_dim = config["embed_dim"]

vqmodel = vqgan.VQModel(ddconfig, lossconfig, n_embed, embed_dim)
vqmodel.init_from_ckpt("model.ckpt")
vqmodel.to(device)

# clip model setup
clipmodel, preprocess = clip.load("ViT-B/32")
clipmodel.to(device)
clipmodel.eval()

def custom_to_pil(x):
  x = np.clip(x, -1., 1.)
  x = (x + 1.)/2.
  x = (255*x).astype(np.uint8)
  x = Image.fromarray(x)
  if not x.mode == "RGB":
    x = x.convert("RGB")
  return x

def decode_from_codes(codes):
  z_q = vqmodel.quantize.embedding(codes)
  j = vqmodel.decode(z_q.reshape(1, 16, 16, 8).permute(0, 3, 1, 2))
  np_img = j[0].permute(1, 2, 0).detach().cpu().numpy()
  return np_img

def get_clip_score(pil_img, prompt):
    image_input = preprocess(pil_img).unsqueeze(0).to(device)
    txt_tokens = clip.tokenize([prompt]).to(device)

    with torch.no_grad():
      img_features = clipmodel.encode_image(image_input)
      txt_features = clipmodel.encode_text(txt_tokens)

    img_features /= img_features.norm(dim=-1, keepdim=True)
    txt_features /= txt_features.norm(dim=-1, keepdim=True)
    clipscore = torch.matmul(img_features, txt_features.T).item()
    return clipscore

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid

def generate_images(prompt, num_imgs, reranking_multiplier, top_p, top_k):
  print(f"Generating (GPT)")

  imgs = []
  encoded_input = tokenizer(prompt.lower() + "<|endoftext|>", return_tensors='pt').to(device)
  num_total_imgs = num_imgs * reranking_multiplier

  with torch.no_grad():
    output = model.generate(**encoded_input, max_new_tokens=256, do_sample=True, top_p=top_p, top_k=top_k, num_return_sequences=num_total_imgs)

  print("Decoding (VQGAN)")
  for item in output:
    img = item[-256:]
    imgs.append(custom_to_pil(decode_from_codes(img-2)))

  # uncomment to view images without contrastive reranking
  #image_grid(imgs, rows=num_imgs, cols=reranking_multiplier)

  print("Reranking (CLIP)")
  best_imgs = sorted(imgs, key=lambda x: get_clip_score(x, prompt), reverse=True)[:num_imgs]
  return best_imgs

"""## Text to image

Run the generator.
"""

#@title Generate images
prompt = "beautiful sunset and palm trees at the dreamy beaches of the caribbean islands" #@param
number_of_images = 4 #@param
reranking_multiplier = 16 #@param
top_p = 0.92 #@param
top_k = 0 #@param

result = generate_images(prompt, number_of_images, reranking_multiplier, top_p, top_k)
print("Done, displaying results")
image_grid(result, rows=1, cols=number_of_images)